'''
6/21/2022

Okay, so I need to figure out how I want the update to work.
There's two types of training.
One is in the "real" game.
The other is in the fake games against random AIs.

So what it should keep track of is the sequence of scenarios, the move it made, and the score at the end.
And also know what mode it's in (for the correct factor to be used).

Then, you also need to do explore exploit as well, but only in the "real" game.

So what it should do is train by testing each move and playing randomly afterwards and updating accordingly.
Then, after all those updates have happened, explore/exploit, and keep track of all those moves.
Then, update the real game.

I still need to do the win function, assign scores when there's an end, actually update based on those scores, and code stuff for a random AI and human to play.
'''

'''
6/22/2022

It's not broken - it's just that I changed the formula.
Apparently using multiplication rather than exponentiation on the discount leads to better percieved performance.
This is because it would actively avoid earlier moves that it lost in before.
Now, it may come back to them if its games against the fake AI are good enough.

So, I think I should raise the default discount.
I also could raise the impact on human games to be far more than the fake monte carlo games.
But I think the default discount raise should be enough.
'''